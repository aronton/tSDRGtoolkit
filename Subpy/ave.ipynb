{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9811cd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PBC', 'Jdis030', 'Dim000', 'L64', 'P10', 'm40', 'ZL', 1, 10000), ('PBC', 'Jdis030', 'Dim000', 'L128', 'P10', 'm40', 'ZL', 1, 10000), ('PBC', 'Jdis030', 'Dim000', 'L192', 'P10', 'm40', 'ZL', 1, 10000), ('PBC', 'Jdis030', 'Dim000', 'L256', 'P10', 'm40', 'ZL', 1, 10000), ('PBC', 'Jdis030', 'Dim000', 'L320', 'P10', 'm40', 'ZL', 1, 10000), ('PBC', 'Jdis030', 'Dim000', 'L384', 'P10', 'm40', 'ZL', 1, 10000), ('PBC', 'Jdis030', 'Dim000', 'L448', 'P10', 'm40', 'ZL', 1, 10000), ('PBC', 'Jdis030', 'Dim000', 'L512', 'P10', 'm40', 'ZL', 1, 10000)]\n",
      "---------------------col--------------------\n",
      "\n",
      "資料已排序，無需排序。\n",
      "✅ 立即取得鎖\n",
      "檔案已鎖定 [WRITE] groupTarPath: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/PBC/Jdis030/Dim000/L64_P10_m40/ZL_L64_P10_m40_Jdis030_Dim000.txt, s1:1, 目前進程 PID: 2711533\n",
      "✅ 檔案已解鎖\n",
      "資料已排序，無需排序。\n",
      "✅ 立即取得鎖\n",
      "檔案已鎖定 [WRITE] groupTarPath: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/PBC/Jdis030/Dim000/L256_P10_m40/ZL_L256_P10_m40_Jdis030_Dim000.txt, s1:1, 目前進程 PID: 2711536\n",
      "✅ 檔案已解鎖\n",
      "資料已排序，無需排序。\n",
      "✅ 立即取得鎖\n",
      "檔案已鎖定 [WRITE] groupTarPath: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/PBC/Jdis030/Dim000/L384_P10_m40/ZL_L384_P10_m40_Jdis030_Dim000.txt, s1:1, 目前進程 PID: 2711538\n",
      "✅ 檔案已解鎖\n",
      "資料已排序，無需排序。\n",
      "✅ 立即取得鎖\n",
      "檔案已鎖定 [WRITE] groupTarPath: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/PBC/Jdis030/Dim000/L128_P10_m40/ZL_L128_P10_m40_Jdis030_Dim000.txt, s1:1, 目前進程 PID: 2711534\n",
      "✅ 檔案已解鎖\n",
      "資料已排序，無需排序。\n",
      "✅ 立即取得鎖\n",
      "檔案已鎖定 [WRITE] groupTarPath: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/PBC/Jdis030/Dim000/L512_P10_m40/ZL_L512_P10_m40_Jdis030_Dim000.txt, s1:1, 目前進程 PID: 2711540\n",
      "✅ 檔案已解鎖\n",
      "Execution time: 308.11560676502995 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import timeit\n",
    "import numpy as np\n",
    "import sys\n",
    "import tarfile\n",
    "import datetime\n",
    "import multiprocessing\n",
    "import scriptCreator\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import fcntl\n",
    "\n",
    "\n",
    "\n",
    "dicosPath = \"/ceph/work/NTHU-qubit/LYT/tSDRG_random\"\n",
    "scopionPath = \"/home/aronton/tSDRG_random\"\n",
    "\n",
    "if os.path.isdir(dicosPath):\n",
    "    tSDRG_path = dicosPath\n",
    "    group_path = dicosPath\n",
    "    \n",
    "if os.path.isdir(scopionPath):\n",
    "    tSDRG_path = scopionPath\n",
    "    group_path = scopionPath\n",
    "    \n",
    "sourcelist = {\"ZL\":\"ZL.csv\", \"energy\":\"energy.csv\", \"seed\":\"s_re_seed.csv\",\\\n",
    "    \"corr1\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"corr1.csv\"]), \"corr2\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"corr2.csv\"]),\\\n",
    "    \"ZLI\":\"ZLI.csv\", \"ZLC\":\"ZLC.csv\", \"w_loc\":\"w_loc.csv\", \"J_list\":\"J_list.csv\", \"dimerization\":\"dimerization.csv\",\\\n",
    "    \"string\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"string.csv\"])\n",
    "    }\n",
    "\n",
    "grouplist = {\"ZL\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"ZL.txt\"]), \"energy\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"energy.txt\"]),\\\n",
    "    \"corr1\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"corr1.txt\"]), \"corr2\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"corr2.txt\"]),\\\n",
    "    \"ZLI\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"ZLI.txt\"]), \"ZLC\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"ZLC.txt\"]),\\\n",
    "    \"w_loc\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"w_loc.txt\"]), \"J_list\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"J_list.txt\"]),\\\n",
    "    \"string\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"string.txt\"]), \"seed\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"seed.txt\"]),\\\n",
    "    \"dimerization\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"dimerization.txt\"])\n",
    "    }\n",
    "\n",
    "tarlist = {\n",
    "    \"ZL\":\"_\".join([\"ZL\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re.txt\"]),\\\n",
    "    \"energy\":\"_\".join([\"energy\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re.txt\"]),\\\n",
    "    \"corr1\":\"_\".join([\"corr1\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re.txt\"]),\\\n",
    "    \"corr2\":\"_\".join([\"corr2\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re.txt\"]),\\\n",
    "    \"ZLI\":\"_\".join([\"ZLI\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re.txt\"]),\\\n",
    "    \"ZLC\":\"_\".join([\"ZLC\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re.txt\"]),\\\n",
    "    \"w_loc\":\"_\".join([\"w_loc\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re.txt\"]),\\\n",
    "    \"J_list\":\"_\".join([\"J_list\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re.txt\"]),\\\n",
    "    \"string\":\"_\".join([\"string\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re.txt\"]),\\\n",
    "    \"seed\":\"_\".join([\"seed\",\"L_re\",\"P_re\",\"m_re.txt\"]),\n",
    "    \"dimerization\":\"_\".join([\"dimerization\",\"L_re\",\"P_re\",\"m_re.txt\"])\n",
    "    }\n",
    "            \n",
    "\n",
    "def checkInside(s, f, sample, phys):\n",
    "    with open(f,\"r\") as a:\n",
    "        a = a.readlines()    \n",
    "        if a[0].strip() == phys:\n",
    "            del a[0]\n",
    "        data = [(v.split(\":\")[0].strip(),(v.split(\":\")[1].replace(\"\\n\",\" \").strip())) for i,v in enumerate(a)]\n",
    "        sorted_data = sorted(data, key=lambda x: int(x[0]))\n",
    "    if s == sorted_data[sample+1][1]:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def compare(f1, f2, sample):\n",
    "    # 讀取兩個檔案\n",
    "    try:\n",
    "        with open(f1, \"r\") as file1:\n",
    "            a = [line.strip() for line in file1 if line.strip()]\n",
    "        with open(f2, \"r\") as file2:\n",
    "            b = [line.strip() for line in file2 if line.strip()]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"檔案不存在：{f1} 或 {f2}\")\n",
    "        return False\n",
    "\n",
    "    # 若有任一檔案為空，直接判定不同\n",
    "    if not a or not b:\n",
    "        return False\n",
    "\n",
    "    # 兩邊都很短（1-2 行），直接比對整體內容\n",
    "    if len(a) <= 2 and len(b) <= 2:\n",
    "        return a == b\n",
    "\n",
    "    # 兩邊都有多行，直接比對整體內容\n",
    "    if len(a) > 2 and len(b) > 2:\n",
    "        return a == b\n",
    "\n",
    "    # ---- 以下為不對稱比對情況 ----\n",
    "\n",
    "    # 把 a 設為短的那一份，b 為長的（方便處理）\n",
    "    if len(a) > len(b):\n",
    "        a, b = b, a  # swap\n",
    "\n",
    "    # 若 a 有 2 行，先刪掉標題行\n",
    "    if len(a) == 2:\n",
    "        data1 = a[1]\n",
    "    else:\n",
    "        data1 = a[0]\n",
    "\n",
    "    # 移除 b 的標題行\n",
    "    b = b[1:]\n",
    "\n",
    "    # 在 b 裡搜尋 data1 對應到的 sample 名稱\n",
    "    for line in b:\n",
    "        if data1 in line:\n",
    "            parts = line.split(\":\")\n",
    "            if parts[0] == sample:\n",
    "                return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "# def compare(f1,f2,sample):\n",
    "#     with open(f1,\"r\") as a:\n",
    "#         a = a.readlines()\n",
    "#     with open(f2,\"r\") as b:\n",
    "#         b = b.readlines()\n",
    "#     if (len(a) <= 2) and (len(b) <=2 ):\n",
    "#         return (a == b)\n",
    "#     elif (len(a) > 2) and (len(b) > 2 ):\n",
    "#         return (a == b)\n",
    "#     elif (len(a) <= 2) and (len(b) > 2 ):\n",
    "#         if len(a) == 2:\n",
    "#             del a[0]\n",
    "#             data1 = a[0].strip()\n",
    "#             # print(data1)\n",
    "#         else:\n",
    "#             data1 = a[0].strip()\n",
    "#             # print(data1)\n",
    "#         del b[0]\n",
    "        \n",
    "#         for i,v in enumerate(b):\n",
    "#             # print(v.split(\":\")[0])\n",
    "#             # print(v.split(\":\")[1])\n",
    "#             if data1 in v.strip():\n",
    "#                 if v.split(\":\")[0] == sample:\n",
    "#                     return True\n",
    "#         return False\n",
    "#     elif (len(a) > 2) and (len(b) < 2 ):\n",
    "#         if len(b) == 2:\n",
    "#             del b[0]\n",
    "#             data1 = b[0].strip()\n",
    "#         else:\n",
    "#             data1 = b[0].strip()\n",
    "#         del a[0]\n",
    "#         for i,v in enumerate(a):\n",
    "#             if data1 in v.strip():\n",
    "#                 if v.split(\":\")[0] == sample:\n",
    "#                     return True\n",
    "#         return False\n",
    "def checkFileNum(dirpath):\n",
    "    folder = Path(dirpath) \n",
    "    file_count = sum(1 for f in folder.iterdir() if f.is_file())\n",
    "    return file_count\n",
    "\n",
    "def creatName(BC, J, D, L, P, m, phys):\n",
    "    mySourceName = sourcelist[phys].replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m) \n",
    "    myTargetName = tarlist[phys].replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m) \n",
    "    groupSourceName = grouplist[phys].replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m) \n",
    "    groupTargetName = tarlist[phys].replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m) \n",
    "    return (mySourceName, groupSourceName, myTargetName, groupTargetName)\n",
    "\n",
    "def creatCpName(BC, J, D, L, P, m, phys):\n",
    "    CpName = newlist[phys].replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m) \n",
    "    return CpName\n",
    "def creatColName(BC, J, D, L, P, m, phys):\n",
    "    colName = collist[phys].replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m) \n",
    "    return colName\n",
    "def creatDir(BC, J, D, L, P, m, phys):\n",
    "    sourcePath = \"/\".join([\"tSDRG\",\"Main_15\",\"data_tar\",\"BC_re\",\"J_re\",\"D_re\",\"L_re_P_re_m_re_s_re\"])\n",
    "    tarPath = \"/\".join([\"tSDRG\",\"Main_15\",\"data_collect_old\",\"BC_re\",\"J_re\",\"D_re\",\"L_re_P_re_m_re\"])\n",
    "    mySourcePathBase = \"/\".join([tSDRG_path,sourcePath])\n",
    "    groupSourcePathBase = \"/\".join([group_path,sourcePath])\n",
    "    myTargetPathBase = \"/\".join([tSDRG_path,tarPath])\n",
    "    groupTargetPathBase = \"/\".join([group_path,tarPath])\n",
    "    # sourcePathBase = f\"{tSDRG_path}/tSDRG/Main_15/data_random/BC_re/J_re/D_re/L_re_P_re_m_re_s_re/\"\n",
    "    # cpPathBase = f\"{group_path}/tSDRG/Main_15/data_random/BC_re/J_re/D_re/L_re_P_re_m_re_s_re/\"\n",
    "    # targetPathBase = f\"{group_path}/tSDRG/Main_15/data_collect/BC_re/J_re/D_re/L_re_P_re_m_re/\"\n",
    "\n",
    "    mySourcePath = mySourcePathBase.replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m)\n",
    "    groupSourcePath = groupSourcePathBase.replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m)\n",
    "    myTargetPath = myTargetPathBase.replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m)\n",
    "    groupTargetPath = groupTargetPathBase.replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m)\n",
    "\n",
    "    # cpPath = cpPathBase.replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m)\n",
    "    # targetPath = targetPathBase.replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m)\n",
    "    \n",
    "    return (mySourcePath, groupSourcePath, myTargetPath, groupTargetPath)\n",
    "\n",
    "def fread(f, phys):\n",
    "    if os.path.exists(f):\n",
    "        with open(f,\"r\") as a:\n",
    "            a = a.readlines()\n",
    "            if len(a) == 0:\n",
    "                return \n",
    "            else:\n",
    "                if phys in a[0].strip():\n",
    "                    del a[0]\n",
    "                a = \"\".join(a)\n",
    "                a = a.replace(\"\\n\",\" \")\n",
    "                return a\n",
    "    else:\n",
    "        return \n",
    "\n",
    "def create_tarball_files(output_filename, file_list):\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        for file in file_list:\n",
    "            tar.add(file, arcname=file)  # arcname 保持原始檔名\n",
    "    print(f\"已打包 {len(file_list)} 個檔案到 {output_filename}\")\n",
    "\n",
    "def kill_files(file_list):\n",
    "    for i,f in enumerate(file_list):\n",
    "        os.system(\"rm \" + f)\n",
    "    return f\"已刪除 {len(file_list)} 個檔案，從{file_list[0]}到{file_list[-1]}\"\n",
    "    \n",
    "def cp_files(file_list):\n",
    "    for i,f in enumerate(file_list):\n",
    "        os.system(\"cp \" + f)\n",
    "    print(f\"已複製 {len(file_list)} 個檔案，從{file_list[0]}到{file_list[-1]}\")\n",
    "\n",
    "\n",
    "\n",
    "def parse_context(context):\n",
    "    \"\"\"\n",
    "    將原始字串解析為鍵值對列表。\n",
    "    \"\"\"\n",
    "    lines = [line.strip() for line in context.strip().split('\\n') if line.strip()]\n",
    "    pairs = []\n",
    "    for line in lines:\n",
    "        if ':' in line:\n",
    "            key_value = line.split(':', 1)\n",
    "            if len(key_value) == 2:\n",
    "                key_str, value = key_value\n",
    "                try:\n",
    "                    key_int = int(key_str.strip())\n",
    "                    pairs.append((key_int, value.strip()))\n",
    "                except ValueError:\n",
    "                    continue\n",
    "    return pairs\n",
    "\n",
    "def is_sorted(pairs):\n",
    "    \"\"\"\n",
    "    檢查鍵值對列表是否已按鍵的升序排序。\n",
    "    \"\"\"\n",
    "    return all(pairs[i][0] <= pairs[i + 1][0] for i in range(len(pairs) - 1))\n",
    "\n",
    "def sort_context(pairs):\n",
    "    \"\"\"\n",
    "    對鍵值對列表按鍵進行排序，並重建為字串格式。\n",
    "    \"\"\"\n",
    "    sorted_pairs = sorted(pairs, key=lambda x: x[0])\n",
    "    s1 = sorted_pairs[0][0]  # 假設第一個鍵是 s1\n",
    "    sorted_lines = [f\"{key}:{value}\" for key, value in sorted_pairs]\n",
    "    return '\\n'.join(sorted_lines), s1\n",
    "\n",
    "def sort_if_needed(context):\n",
    "    \"\"\"\n",
    "    若資料未排序，則進行排序；否則返回原始資料。\n",
    "    \"\"\"\n",
    "    pairs = parse_context(context)\n",
    "    if is_sorted(pairs):\n",
    "        print(\"資料已排序，無需排序。\")\n",
    "        s1 = int(pairs[0][0])  # 假設第一個鍵是 s1\n",
    "        return context, s1\n",
    "    else:\n",
    "        print(\"資料未排序，開始排序。\")\n",
    "        return sort_context(pairs)\n",
    "        \n",
    "def Combine(BC, J, D, L, P, m, phys, s1, s2):\n",
    "    folder = creatDir(BC, J, D, L, P, m, phys)\n",
    "    name = creatName(BC, J, D, L, P, m, phys)\n",
    "\n",
    "    mySourcePath = folder[0] + \"/\" + name[0]\n",
    "    groupSourcePath = folder[1] + \"/\" + name[1]\n",
    "    myTarPath = folder[2] + \"/\" + name[2]\n",
    "    groupTarPath = folder[3] + \"/\" + name[3]\n",
    "\n",
    "    seedArray = list(range(s1, s2 + 1))\n",
    "    # with open(groupTarPath, \"r\") as originFile:\n",
    "    #     originaText = originFile.readlines()\n",
    "    context = \"\"\n",
    "    # print(\"originaText\")\n",
    "    for seed in seedArray:\n",
    "        groupSource = groupSourcePath.replace(\"s_re\", str(seed))\n",
    "        mySource = mySourcePath.replace(\"s_re\", str(seed))\n",
    "        # if f\"{seed}:\" in originaText[seed-1]:\n",
    "        #     continue\n",
    "        if os.path.exists(groupSource) and os.path.exists(mySource):\n",
    "            if compare(groupSource, mySource, seed):                \n",
    "                fcontext = fread(mySource, phys)\n",
    "            else:\n",
    "                # os.remove(groupSource)\n",
    "                shutil.copy(mySource, groupSource)\n",
    "                fcontext = fread(groupSource, phys)\n",
    "        elif os.path.exists(mySource):\n",
    "            os.makedirs(os.path.dirname(groupSource), exist_ok=True)\n",
    "            shutil.copy(mySource, groupSource)\n",
    "            # os.remove(mySource)\n",
    "            fcontext = fread(groupSource, phys)\n",
    "        elif os.path.exists(groupSource):\n",
    "            fcontext = fread(groupSource, phys)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        if fcontext is not None:\n",
    "            context += f\"{seed}:{fcontext}\\n\"\n",
    "\n",
    "    if context != \"\":\n",
    "        context, s1 = sort_if_needed(context)\n",
    "        \n",
    "        save_context(context, s1, groupTarPath, myTarPath, phys)\n",
    "        # os.makedirs(os.path.dirname(myTarPath), exist_ok=True)\n",
    "\n",
    "\n",
    "def save_context(context, s1, groupTarPath, myTarPath, phys):\n",
    "    if not os.path.exists(groupTarPath):\n",
    "        os.makedirs(os.path.dirname(groupTarPath), exist_ok=True)\n",
    "    if s1 == 1:\n",
    "        context = f\"{phys}\\n{context}\"\n",
    "        # print(f\"[WRITE] groupTarPath: {groupTarPath}, myTarPath: {myTarPath}\")\n",
    "        with open(groupTarPath, \"w\") as f1:\n",
    "            \n",
    "            try:\n",
    "                # 嘗試用非阻塞方式加鎖\n",
    "                fcntl.flock(f1, fcntl.LOCK_EX | fcntl.LOCK_NB)\n",
    "                print(\"✅ 立即取得鎖\")\n",
    "                print(f\"檔案已鎖定 [WRITE] groupTarPath: {groupTarPath}, s1:{s1}, 目前進程 PID: {os.getpid()}\")\n",
    "                f1.write(context)\n",
    "                fcntl.flock(f1, fcntl.LOCK_EX | fcntl.LOCK_NB)\n",
    "                print(\"✅ 檔案已解鎖\")    \n",
    "            except BlockingIOError:\n",
    "                print(\"⏳ 檔案已被鎖住，進入等待模式...\")\n",
    "                fcntl.flock(f1, fcntl.LOCK_EX)  # 這裡才會阻塞，等釋放\n",
    "                print(\"✅ 最終取得鎖\")\n",
    "                print(f\"檔案已鎖定 [WRITE] groupTarPath: {groupTarPath}, s1:{s1}, 目前進程 PID: {os.getpid()}\")\n",
    "                f1.write(context)\n",
    "                fcntl.flock(f1, fcntl.LOCK_UN)\n",
    "                print(\"✅ 檔案已解鎖\")            # f2.write(context)\n",
    "        # with open(groupTarPath, \"w\") as f1, open(myTarPath, \"w\") as f2:\n",
    "        #     f1.write(context)\n",
    "        #     # f2.write(context)\n",
    "    else:\n",
    "        # print(f\"[APPEND] groupTarPath: {groupTarPath}, myTarPath: {myTarPath}\")\n",
    "        with open(groupTarPath, \"a\") as f1:\n",
    "            \n",
    "            try:\n",
    "                # 嘗試用非阻塞方式加鎖\n",
    "                fcntl.flock(f1, fcntl.LOCK_EX | fcntl.LOCK_NB)\n",
    "                print(\"✅ 立即取得鎖\")\n",
    "                print(f\"檔案已鎖定 [APPEND] groupTarPath: {groupTarPath}, s1:{s1}, 目前進程 PID: {os.getpid()}\")\n",
    "                f1.write(context)\n",
    "                fcntl.flock(f1, fcntl.LOCK_EX | fcntl.LOCK_NB)\n",
    "                print(\"✅ 檔案已解鎖\")    \n",
    "            except BlockingIOError:\n",
    "                print(\"⏳ 檔案已被鎖住，進入等待模式...\")\n",
    "                fcntl.flock(f1, fcntl.LOCK_EX)  # 這裡才會阻塞，等釋放\n",
    "                print(\"✅ 最終取得鎖\")\n",
    "                print(f\"檔案已鎖定 [APPEND] groupTarPath: {groupTarPath}, s1:{s1}, 目前進程 PID: {os.getpid()}\")\n",
    "                f1.write(context)\n",
    "                fcntl.flock(f1, fcntl.LOCK_UN)\n",
    "                print(\"✅ 檔案已解鎖\") \n",
    "    # # print(\"originaText222\")\n",
    "    # if s1 == 1:\n",
    "    #     context = f\"{phys}\\n{context}\"\n",
    "    #     print(f\"[WRITE] groupTarPath: {groupTarPath}, myTarPath: {myTarPath}\")\n",
    "    #     with open(groupTarPath, \"w\") as f1:\n",
    "    #         f1.write(context)\n",
    "    #         # f2.write(context)\n",
    "    #     # with open(groupTarPath, \"w\") as f1, open(myTarPath, \"w\") as f2:\n",
    "    #     #     f1.write(context)\n",
    "    #     #     # f2.write(context)\n",
    "    # else:\n",
    "    #     print(f\"[APPEND] groupTarPath: {groupTarPath}, myTarPath: {myTarPath}\")\n",
    "    #     with open(groupTarPath, \"a\") as f1:\n",
    "    #         f1.write(context)\n",
    "    #             # f2.write(context)         \n",
    "    #         # with open(groupTarPath, \"a\") as f1, open(myTarPath, \"a\") as f2:\n",
    "    #         #     f1.write(context)\n",
    "    #         #     # f2.write(context)          \n",
    "def average(BC, J, D, L, P, m, phys, s1, s2):\n",
    "    folder = creatDir(BC, J, D, L, P, m, phys)\n",
    "    name = creatName(BC, J, D, L, P, m, phys)\n",
    "\n",
    "    # mySourcePath = folder[0] + \"/\" + name[0]\n",
    "    # groupSourcePath = folder[1] + \"/\" + name[1]\n",
    "    myTarPath = folder[2] + \"/\" + name[2]\n",
    "    groupTarPath = folder[3] + \"/\" + name[3]\n",
    "    \n",
    "    with open(myTarPath,\"r\") as a:\n",
    "        a = a.readlines()\n",
    "        if phys in a[0].strip():\n",
    "            del a[0]\n",
    "        metaContext = {}\n",
    "        for s in a:\n",
    "            s = s.strip()\n",
    "            sNum = int(s[0].split(\":\")[0])\n",
    "            if sNum not in metaContext:\n",
    "                metaContext[sNum] = {}\n",
    "                \n",
    "            del s[0]\n",
    "            s = s.replace(\" \")\n",
    "            del s[0]\n",
    "            for corr in s:\n",
    "                if int(corr[1]) - int(corr[0]) not in dic:\n",
    "                    metaContext[sNum][int(corr[1]) - int(corr[0])] = [float(corr[2])]\n",
    "                else:\n",
    "                    metaContext[sNum][int(corr[1]) - int(corr[0])].append(float(corr[2]))\n",
    "\n",
    "        \n",
    "def parameter_read_dict(filename):\n",
    "    parameters = {}\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                if ':' in line:\n",
    "                    key, value = line.split(':', 1)\n",
    "                    key = key.strip()\n",
    "                    value = value.strip()\n",
    "                    if key:\n",
    "                        parameters[key] = value\n",
    "    except FileNotFoundError:\n",
    "        print(f\"無法開啟檔案: {filename}\")\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    file = sys.argv[1]\n",
    "    arg = []\n",
    "    # Jstr = [f\"Jdis{str(i).zfill(3)}\" for i in range(int(J),int(J)+1)]\n",
    "    Jstr = [f\"Jdis{str(i).zfill(3)}\" for i in range(30,31,50)]\n",
    "\n",
    "    Dstr = [f\"Dim{str(i).zfill(3)}\" for i in range(101)]\n",
    "    Lstr = [f\"L{num}\" for num in range(31, 255, 32)]  # 只有 L512\n",
    "    Lstr = [f\"L{num}\" for num in range(64, 550, 64)]  # 只有 L512\n",
    "    # a = scriptCreator.para(\"read\",file)\n",
    "    # parameterlist = a.para\n",
    "    # para=scriptCreator.paraList1(parameterlist[\"L\"],parameterlist[\"J\"],parameterlist[\"D\"],parameterlist[\"S\"])\n",
    "    # BC = parameterlist[\"BC\"]\n",
    "    # Pdis = parameterlist[\"Pdis\"]\n",
    "    # chi = \"m\" + str(parameterlist[\"chi\"])\n",
    "    # s1 = int(parameterlist[\"S\"][\"S1\"])\n",
    "    # s2 = int(parameterlist[\"S\"][\"S2\"])\n",
    "    # s1 = int(sys.argv[2])\n",
    "    # s2 = int(sys.argv[3])\n",
    "    BC = \"PBC\"\n",
    "    Pdis = 10\n",
    "    chi = \"m40\"\n",
    "    if BC == \"PBC\":\n",
    "        s_list = [\"ZL\",\"corr1\",\"corr2\",\"string\",\"J_list\",\"energy\",\"dimerization\",\"w_loc\",\"seed\"]\n",
    "        s_list = [\"ZL\"]\n",
    "    else:\n",
    "        s_list = [\"ZL\",\"corr1\",\"corr2\",\"J_list\",\"energy\",\"dimerization\",\"w_loc\",\"seed\"]\n",
    "        \n",
    "    # for s in s_list:\n",
    "    #     for L in para.L_str:\n",
    "    #         for J in para.J_str:\n",
    "    #                 arg.append((BC, J, para.D_str[0], L, f\"P{Pdis}\", f\"{chi}\", s, s1, s2))\n",
    "    s1 = 1\n",
    "    s2 = 10000  \n",
    "    for s in s_list:\n",
    "        for L in Lstr:\n",
    "            for J in Jstr:\n",
    "                    arg.append((BC, J, Dstr[0], L, f\"P{Pdis}\", f\"{chi}\", s, s1, s2))\n",
    "    print(arg)         \n",
    "\n",
    "    def fun(arg):\n",
    "        print(\"---------------------col--------------------\\n\")\n",
    "        with multiprocessing.Pool(processes=10) as pool:\n",
    "            results1 = pool.starmap(Combine, arg)\n",
    "        # print(\"---------------------del--------------------\\n\")\n",
    "        # with multiprocessing.Pool(processes=20) as pool:\n",
    "        #     results1 = pool.starmap(checkAndDelete, arg)\n",
    "            \n",
    "    # 計算函數執行時間\n",
    "    execution_time = timeit.timeit(lambda: fun(arg), number=1)\n",
    "\n",
    "    # 執行並顯示結果\n",
    "    # results1, results2 = fun(arg)\n",
    "    print(f\"Execution time: {execution_time} seconds\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3735300b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0003649455223832696 2000 0.2653707850853128\n",
      "0.003191211143744681 5000 0.3512899960553954\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/PBC/Jdis030/Dim000/L192_P10_m40/ZL_L192_P10_m40_Jdis030_Dim000.txt\n",
      "Error: No data found for PBC, Jdis030, Dim000, L192, P10, m40, ZL\n",
      "0.004632788702475768 5000 0.42475869585796416\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/PBC/Jdis030/Dim000/L320_P10_m40/ZL_L320_P10_m40_Jdis030_Dim000.txt\n",
      "Error: No data found for PBC, Jdis030, Dim000, L320, P10, m40, ZL\n",
      "-0.008319057236040553 5000 0.4652774721489594\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/PBC/Jdis030/Dim000/L448_P10_m40/ZL_L448_P10_m40_Jdis030_Dim000.txt\n",
      "Error: No data found for PBC, Jdis030, Dim000, L448, P10, m40, ZL\n",
      "-0.0082047040317973 5000 0.4766786822883327\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import timeit\n",
    "import sys\n",
    "import tarfile\n",
    "import datetime\n",
    "import multiprocessing\n",
    "import scriptCreator\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "dicosPath = \"/ceph/work/NTHU-qubit/LYT/tSDRG_random\"\n",
    "scopionPath = \"/home/aronton/tSDRG_random\"\n",
    "\n",
    "if os.path.isdir(dicosPath):\n",
    "    tSDRG_path = dicosPath\n",
    "    group_path = dicosPath\n",
    "    \n",
    "if os.path.isdir(scopionPath):\n",
    "    tSDRG_path = scopionPath\n",
    "    group_path = scopionPath\n",
    "    \n",
    "sourcelist = {\"ZL\":\"ZL.csv\", \"energy\":\"energy.csv\", \"seed\":\"s_re_seed.csv\",\\\n",
    "    \"corr1\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"corr1.csv\"]), \"corr2\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"corr2.csv\"]),\\\n",
    "    \"ZLI\":\"ZLI.csv\", \"ZLC\":\"ZLC.csv\", \"w_loc\":\"w_loc.csv\", \"J_list\":\"J_list.csv\", \"dimerization\":\"dimerization.csv\",\\\n",
    "    \"string\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"string.csv\"])\n",
    "    }\n",
    "\n",
    "grouplist = {\"ZL\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"ZL.txt\"]), \"energy\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"energy.txt\"]),\\\n",
    "    \"corr1\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"corr1.txt\"]), \"corr2\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"corr2.txt\"]),\\\n",
    "    \"ZLI\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"ZLI.txt\"]), \"ZLC\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"ZLC.txt\"]),\\\n",
    "    \"w_loc\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"w_loc.txt\"]), \"J_list\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"J_list.txt\"]),\\\n",
    "    \"string\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"string.txt\"]), \"seed\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"seed.txt\"]),\\\n",
    "    \"dimerization\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"dimerization.txt\"])\n",
    "    }\n",
    "\n",
    "tarlist = {\n",
    "    \"ZL\":\"_\".join([\"ZL\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re.txt\"]),\\\n",
    "    \"energy\":\"_\".join([\"energy\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re.txt\"]),\\\n",
    "    \"corr1\":\"_\".join([\"corr1\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re.txt\"]),\\\n",
    "    \"corr2\":\"_\".join([\"corr2\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re.txt\"]),\\\n",
    "    \"ZLI\":\"_\".join([\"ZLI\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re.txt\"]),\\\n",
    "    \"ZLC\":\"_\".join([\"ZLC\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re.txt\"]),\\\n",
    "    \"w_loc\":\"_\".join([\"w_loc\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re.txt\"]),\\\n",
    "    \"J_list\":\"_\".join([\"J_list\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re.txt\"]),\\\n",
    "    \"string\":\"_\".join([\"string\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re.txt\"]),\\\n",
    "    \"seed\":\"_\".join([\"seed\",\"L_re\",\"P_re\",\"m_re.txt\"]),\n",
    "    \"dimerization\":\"_\".join([\"dimerization\",\"L_re\",\"P_re\",\"m_re.txt\"])\n",
    "    }\n",
    "\n",
    "metalist = {\n",
    "    \"ZL\":\"_\".join([\"ZL\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re.txt\"]),\\\n",
    "    \"gap\":\"_\".join([\"energy\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re.txt\"]),\\\n",
    "    \"corr1\":\"_\".join([\"corr1\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re\",\"dx_re.txt\"]),\\\n",
    "    \"corr2\":\"_\".join([\"corr2\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re\",\"dx_re.txt\"]),\\\n",
    "    }\n",
    "\n",
    "metaDislist = {\n",
    "    \"ZL\":\"_\".join([\"ZL_dis\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re.txt\"]),\\\n",
    "    \"gap\":\"_\".join([\"gap_dis\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re.txt\"]),\\\n",
    "    \"corr1\":\"_\".join([\"corr1_dis\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re\",\"dx_re.txt\"]),\\\n",
    "    \"corr2\":\"_\".join([\"corr2_dis\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re\",\"dx_re.txt\"]),\\\n",
    "    }\n",
    "\n",
    "\n",
    "def creatName(BC, J, D, L, P, m, phys):\n",
    "    mySourceName = sourcelist[phys].replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m) \n",
    "    myTargetName = tarlist[phys].replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m) \n",
    "    groupSourceName = grouplist[phys].replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m) \n",
    "    groupTargetName = tarlist[phys].replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m) \n",
    "    groupAveName = metalist[phys].replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m)\n",
    "    groupDisName = metaDislist[phys].replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m)\n",
    "    return (mySourceName, groupSourceName, myTargetName, groupTargetName, groupAveName, groupDisName)\n",
    "\n",
    "# def creatCpName(BC, J, D, L, P, m, phys):\n",
    "#     CpName = newlist[phys].replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m) \n",
    "#     return CpName\n",
    "# def creatColName(BC, J, D, L, P, m, phys):\n",
    "#     colName = collist[phys].replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m) \n",
    "#     return colName\n",
    "def creatDir(BC, J, D, L, P, m, phys):\n",
    "    avePath = \"/\".join([\"tSDRG\",\"Main_15\",\"metadata_old\",\"BC_re\",\"J_re\",\"D_re\",\"L_re_P_re_m_re_s_re\"])\n",
    "    sourcePath = \"/\".join([\"tSDRG\",\"Main_15\",\"data_tar\",\"BC_re\",\"J_re\",\"D_re\",\"L_re_P_re_m_re_s_re\"])\n",
    "    tarPath = \"/\".join([\"tSDRG\",\"Main_15\",\"data_collect_old\",\"BC_re\",\"J_re\",\"D_re\",\"L_re_P_re_m_re\"])\n",
    "    mySourcePathBase = \"/\".join([tSDRG_path,sourcePath])\n",
    "    groupSourcePathBase = \"/\".join([group_path,sourcePath])\n",
    "    myTargetPathBase = \"/\".join([tSDRG_path,tarPath])\n",
    "    groupTargetPathBase = \"/\".join([group_path,tarPath])\n",
    "    avePathBase = \"/\".join([tSDRG_path,avePath])\n",
    "    # disPathBase = \"/\".join([tSDRG_path,avePath])\n",
    "    # sourcePathBase = f\"{tSDRG_path}/tSDRG/Main_15/data_random/BC_re/J_re/D_re/L_re_P_re_m_re_s_re/\"\n",
    "    # cpPathBase = f\"{group_path}/tSDRG/Main_15/data_random/BC_re/J_re/D_re/L_re_P_re_m_re_s_re/\"\n",
    "    # targetPathBase = f\"{group_path}/tSDRG/Main_15/data_collect/BC_re/J_re/D_re/L_re_P_re_m_re/\"\n",
    "\n",
    "    mySourcePath = mySourcePathBase.replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m)\n",
    "    groupSourcePath = groupSourcePathBase.replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m)\n",
    "    myTargetPath = myTargetPathBase.replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m)\n",
    "    groupTargetPath = groupTargetPathBase.replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m)\n",
    "    myAvePath = avePathBase.replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m).replace(\"s_re\", \"meta\")\n",
    "    myDisPath = avePathBase.replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m).replace(\"s_re\", \"dis\")\n",
    "    # cpPath = cpPathBase.replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m)\n",
    "    # targetPath = targetPathBase.replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m)\n",
    "    \n",
    "    return (mySourcePath, groupSourcePath, myTargetPath, groupTargetPath, myAvePath, myDisPath)\n",
    "\n",
    "def gapAverage(BC, J, D, L, P, m, phys):\n",
    "    folder = creatDir(BC, J, D, L, P, m, phys)\n",
    "    name = creatName(BC, J, D, L, P, m, phys)\n",
    "    myTarPath = folder[2] + \"/\" + name[2]\n",
    "\n",
    "    gaplist = []\n",
    "    try:\n",
    "        with open(myTarPath, \"r\") as targetFile:\n",
    "            collect = targetFile.readlines()\n",
    "            collect = [line.strip() for line in collect]\n",
    "            if collect and collect[0] == \"energy\":\n",
    "                del collect[0]\n",
    "            for line in collect:\n",
    "                line = line.split(\":\")[-1].split(\" \")\n",
    "                try:\n",
    "                    gap = float(line[1]) - float(line[0])\n",
    "                    gaplist.append(gap)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {myTarPath}\")\n",
    "        return False, False, False\n",
    "    save_gapDistribute(gaplist, BC, J, D, L, P, m, phys)\n",
    "    gapAve = np.mean(gaplist)\n",
    "    sample = len(gaplist) \n",
    "    error = np.std(gaplist, ddof=1)\n",
    "\n",
    "    return gapAve, sample, error\n",
    "\n",
    "def save_gapDistribute(gaplist, BC, J, D, L, P, m, phys):\n",
    "    folder = creatDir(BC, J, D, L, P, m, phys)\n",
    "    name = creatName(BC, J, D, L, P, m, phys)\n",
    "    # print( f\"folder[4]:{folder[4]}\")\n",
    "    gapDisBase = folder[5] + \"/\" + name[5]\n",
    "    context = \"ground_state_gap\\n\"\n",
    "    for i, value in enumerate(gaplist):\n",
    "        context += f\"{value}\\n\"\n",
    "    \n",
    "    if context == \"ground_state_gap\\n\":\n",
    "        return\n",
    "    else:\n",
    "        if not os.path.exists(gapDisBase):\n",
    "            os.makedirs(os.path.dirname(gapDisBase), exist_ok=True)\n",
    "        with open(gapDisBase, \"w\") as targetFile:\n",
    "            targetFile.write(context)\n",
    "\n",
    "def save_gap(BC, J, D, L, P, m, phys):\n",
    "    gapAve, sample, error = gapAverage(BC, J, D, L, P, m, phys)\n",
    "    if gapAve == False:\n",
    "        print(f\"Error: No data found for {BC}, {J}, {D}, {L}, {P}, {m}, {phys}\")\n",
    "        return\n",
    "    folder = creatDir(BC, J, D, L, P, m, phys)\n",
    "    name = creatName(BC, J, D, L, P, m, phys)\n",
    "    # print( f\"folder[4]:{folder[4]}\")\n",
    "    myTarPath = folder[4] + \"/\" + name[4]\n",
    "    # print(f\"myTarPath:{myTarPath}\")\n",
    "    if not os.path.exists(myTarPath):\n",
    "        os.makedirs(os.path.dirname(myTarPath), exist_ok=True)\n",
    "    with open(myTarPath, \"w\") as targetFile:\n",
    "        targetFile.write(f\"ground_state_energy, sample, error\\n{gapAve}, {sample}, {error/math.sqrt(sample)}\")\n",
    "            \n",
    "def corrAverage(BC, J, D, L, P, m, phys):\n",
    "    folder = creatDir(BC, J, D, L, P, m, phys)\n",
    "    name = creatName(BC, J, D, L, P, m, phys)\n",
    "    myTarPath = folder[2] + \"/\" + name[2]\n",
    "\n",
    "    corrDic = {}\n",
    "    try:\n",
    "        with open(myTarPath, \"r\") as targetFile:\n",
    "            collect = targetFile.readlines()\n",
    "            collect = [line.strip() for line in collect]\n",
    "            if collect and collect[0] == \"corr1\":\n",
    "                del collect[0]\n",
    "            for line in collect:\n",
    "                line = line.split(\":\")[-1].split(\" \")\n",
    "                for data in line:\n",
    "                    if data:\n",
    "                        parts = data.split(\",\")\n",
    "                        if len(parts) >= 3:\n",
    "                            try:\n",
    "                                x1, x2 = int(parts[0]), int(parts[1])\n",
    "                                corr = float(parts[2])\n",
    "                                key = x2 - x1\n",
    "                                if key not in corrDic:\n",
    "                                    corrDic[key] = []\n",
    "                                corrDic[key].append(corr)\n",
    "                            except ValueError:\n",
    "                                continue  # 忽略無法轉換的數據\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {myTarPath}\")\n",
    "        return False, False, False\n",
    "    save_corrDistribute(corrDic, BC, J, D, L, P, m, phys)\n",
    "    corr = {}\n",
    "    sample = {}\n",
    "    error = {}\n",
    "    for key, values in corrDic.items():\n",
    "        sample[key] = len(values)\n",
    "        corr[key] = np.mean(values)\n",
    "        error[key] = np.std(values, ddof=1) if len(values) > 1 else 0.0\n",
    "\n",
    "    return corr, sample, error\n",
    "\n",
    "def save_corrDistribute(corrDic, BC, J, D, L, P, m, phys):\n",
    "    folder = creatDir(BC, J, D, L, P, m, phys)\n",
    "    name = creatName(BC, J, D, L, P, m, phys)\n",
    "    # print( f\"folder[4]:{folder[4]}\")\n",
    "    corrDisBase = folder[5] + \"/\" + name[5]\n",
    "    for key, values in corrDic.items():\n",
    "        context = \"\"\n",
    "        if BC == \"OBC\":\n",
    "            context = f\"C_etoe=<S(0)S(dx={key})>\\n\"\n",
    "        elif BC == \"PBC\":\n",
    "            context = f\"C_bulk=<S(0)S(dx={key})>\\n\"\n",
    "        corrDisPath = corrDisBase.replace(\"dx_re\", f\"dx={key}\")\n",
    "        for value in values:\n",
    "            context += f\"{value}\\n\"\n",
    "        \n",
    "        if context == f\"C_etoe=<S(0)S(dx={key})>\\n\" or context ==  f\"C_bulk=<S(0)S(dx={key})>\\n\":\n",
    "            continue\n",
    "        else:\n",
    "            if not os.path.exists(corrDisPath):\n",
    "                os.makedirs(os.path.dirname(corrDisPath), exist_ok=True)\n",
    "            with open(corrDisPath, \"w\") as targetFile:\n",
    "                targetFile.write(context)\n",
    "\n",
    "def save_corr(BC, J, D, L, P, m, phys):\n",
    "    corr, sample, error = corrAverage(BC, J, D, L, P, m, phys)\n",
    "    if corr == False:\n",
    "        print(f\"Error: No data found for {BC}, {J}, {D}, {L}, {P}, {m}, {phys}\")\n",
    "        return\n",
    "    folder = creatDir(BC, J, D, L, P, m, phys)\n",
    "    name = creatName(BC, J, D, L, P, m, phys)\n",
    "    # print( f\"folder[4]:{folder[4]}\")\n",
    "    myTarPathBase = folder[4] + \"/\" + name[4]\n",
    "    # print(f\"myTarPath:{myTarPath}\")\n",
    "\n",
    "    for key in corr.keys():\n",
    "        if corr[key] == None:\n",
    "            continue\n",
    "        myTarPath = myTarPathBase.replace(\"dx_re\", f\"dx={key}\")\n",
    "        if BC == \"OBC\":\n",
    "            context = f\"C_etoe=<S(0)S(dx={key})>,sample,errorbar\\n\" + f\"{corr[key]},{sample[key]},{error[key]/math.sqrt(sample[key])}\"\n",
    "        elif BC == \"PBC\":\n",
    "            context = f\"C_bulk=<S(0)S(dx={key})>,sample,errorbar\\n\" + f\"{corr[key]},{sample[key]},{error[key]/math.sqrt(sample[key])}\"\n",
    "        if not os.path.exists(myTarPath):\n",
    "            os.makedirs(os.path.dirname(myTarPath), exist_ok=True)\n",
    "        with open(myTarPath, \"w\") as targetFile:\n",
    "            targetFile.write(context)\n",
    "\n",
    "\n",
    "def ZLAverage(BC, J, D, L, P, m, phys):\n",
    "    folder = creatDir(BC, J, D, L, P, m, phys)\n",
    "    name = creatName(BC, J, D, L, P, m, phys)\n",
    "    myTarPath = folder[2] + \"/\" + name[2]\n",
    "    zllist = []\n",
    "    \n",
    "    try:\n",
    "        if not os.path.exists(myTarPath):\n",
    "            raise FileNotFoundError(f\"{myTarPath} does not exist\")\n",
    "        auto_delete_empty = True\n",
    "        if os.path.getsize(myTarPath) == 0:\n",
    "            if auto_delete_empty:\n",
    "                os.remove(myTarPath)\n",
    "                print(f\"[刪除] 空檔案已刪除：{myTarPath}\")\n",
    "            raise ValueError(\"檔案為空\")\n",
    "        with open(myTarPath, \"r\") as targetFile:\n",
    "            zllist = targetFile.readlines()\n",
    "            if type(zllist[0]) == str or zllist[0] == \"ZL\":\n",
    "                del zllist[0]\n",
    "            zllist = [float(line.split(\":\")[-1]) for line in zllist]\n",
    "        if not zllist:\n",
    "            if auto_delete_empty:\n",
    "                os.remove(myTarPath)\n",
    "                print(f\"[刪除] 空檔案已刪除：{myTarPath}\")\n",
    "            raise ValueError(\"檔案內容為空\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {myTarPath}\")\n",
    "        return False, False, False\n",
    "    \n",
    "    save_zlDistribute(zllist, BC, J, D, L, P, m, phys)\n",
    "    zlAve = np.mean(zllist)\n",
    "    sample = len(zllist)\n",
    "    error = np.std(zllist, ddof=1)\n",
    "\n",
    "    return zlAve, sample, error\n",
    "\n",
    "def save_zlDistribute(zllist, BC, J, D, L, P, m, phys):\n",
    "    folder = creatDir(BC, J, D, L, P, m, phys)\n",
    "    name = creatName(BC, J, D, L, P, m, phys)\n",
    "    # print( f\"folder[4]:{folder[4]}\")\n",
    "    zlDisBase = folder[5] + \"/\" + name[5]\n",
    "    context = \"ZL\\n\"\n",
    "    for i, value in enumerate(zllist):\n",
    "        context += f\"{value}\\n\"\n",
    "    if context == \"ZL\\n\":\n",
    "        return\n",
    "    else:\n",
    "        if not os.path.exists(zlDisBase):\n",
    "            os.makedirs(os.path.dirname(zlDisBase), exist_ok=True)\n",
    "        with open(zlDisBase, \"w\") as targetFile:\n",
    "            targetFile.write(context)\n",
    "\n",
    "def save_ZL(BC, J, D, L, P, m, phys):\n",
    "    zlAve, sample, error = ZLAverage(BC, J, D, L, P, m, phys)\n",
    "    if zlAve == False:\n",
    "        print(f\"Error: No data found for {BC}, {J}, {D}, {L}, {P}, {m}, {phys}\")\n",
    "        return\n",
    "    folder = creatDir(BC, J, D, L, P, m, phys)\n",
    "    name = creatName(BC, J, D, L, P, m, phys)\n",
    "    # print( f\"folder[4]:{folder[4]}\")\n",
    "    myTarPath = folder[4] + \"/\" + name[4]\n",
    "\n",
    "    if not os.path.exists(myTarPath):\n",
    "        os.makedirs(os.path.dirname(myTarPath), exist_ok=True)\n",
    "    with open(myTarPath, \"w\") as targetFile:\n",
    "        targetFile.write(f\"ZL, sample, errorbar\\n{zlAve}, {sample}, {error/math.sqrt(sample)}\")    \n",
    "# def save_gap():\n",
    "\n",
    "def list_txt_files(directory):\n",
    "    txt_files = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                full_path = os.path.join(root, file)\n",
    "                txt_files.append(full_path)\n",
    "    return txt_files\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # file = sys.argv[1]\n",
    "    # arg = []\n",
    "    # a = scriptCreator.para(\"read\",file)\n",
    "    # parameterlist = a.para\n",
    "    # para=scriptCreator.paraList1(parameterlist[\"L\"],parameterlist[\"J\"],parameterlist[\"D\"],parameterlist[\"S\"])\n",
    "    # BC = parameterlist[\"BC\"]\n",
    "    # Pdis = parameterlist[\"Pdis\"]\n",
    "    # chi = \"m\" + str(parameterlist[\"chi\"])\n",
    "    # s1 = int(sys.argv[2])\n",
    "    # s2 = int(sys.argv[3])\n",
    "    # if BC == \"PBC\":\n",
    "    #     s_list = [\"ZL\",\"corr1\",\"corr2\",\"string\",\"J_list\",\"energy\",\"dimerization\",\"w_loc\",\"seed\"]\n",
    "    #     s_list = [\"corr1\",\"corr2\"]\n",
    "    # else:\n",
    "    #     s_list = [\"ZL\",\"corr1\",\"corr2\",\"J_list\",\"energy\",\"dimerization\",\"w_loc\",\"seed\"]\n",
    "    #     s_list = [\"corr1\",\"corr2\"]\n",
    "        \n",
    "    # # for s in s_list:\n",
    "    # for D in para.D_str:\n",
    "    #     for L in para.L_str:\n",
    "    #         for J in para.J_str:\n",
    "    #             # save_corr(BC, J, D, L, f\"P{Pdis}\", f\"{chi}\", \"corr1\")    \n",
    "    #             # save_corr(BC, J, D, L, f\"P{Pdis}\", f\"{chi}\", \"corr2\")\n",
    "    #             # save_gap(BC, J, D, L, f\"P{Pdis}\", f\"{chi}\", \"energy\")   \n",
    "    #             save_ZL(BC, J, D, L, f\"P{Pdis}\", f\"{chi}\", \"ZL\")   \n",
    "    #                 # arg.append((BC, J, para.D_str[0], L, f\"P{Pdis}\", f\"{chi}\", s, s1, s2))\n",
    "    # Jstr = [f\"Jdis{str(i).zfill(3)}\" for i in range(int(J),int(J)+1)]\n",
    "             \n",
    "    # 參數設定\n",
    "    D_i, D_f, D_d = 0, 61, 1\n",
    "    J_i, J_f, J_d = 1, 51, 1\n",
    "    L_i, L_f, L_d = 32, 550, 32                    \n",
    "        \n",
    "    D_list = [f\"Dim{str(i).zfill(3)}\" for i in range(D_i, D_f, D_d)]\n",
    "    J_list = [f\"Jdis{str(i).zfill(3)}\" for i in range(J_i, J_f, J_d)]\n",
    "    L_list = [f\"L{num}\" for num in range(L_i, L_f, L_d)]\n",
    "    BC = \"PBC\"\n",
    "    Pdis = 10\n",
    "    chi = \"m\" + str(40)\n",
    "    s1 = 1\n",
    "    s2 = 10000\n",
    "    for D in D_list:\n",
    "        for L in L_list:\n",
    "            for J in J_list:\n",
    "                # save_corr(BC, J, D, L, f\"P{Pdis}\", f\"{chi}\", \"corr1\")    \n",
    "                # save_corr(BC, J, D, L, f\"P{Pdis}\", f\"{chi}\", \"corr2\")\n",
    "                # save_gap(BC, J, D, L, f\"P{Pdis}\", f\"{chi}\", \"energy\")   \n",
    "                save_ZL(BC, J, D, L, f\"P{Pdis}\", f\"{chi}\", \"ZL\")   \n",
    "    # save_corr(BC, J, para.D_str[0], L, f\"P{Pdis}\", f\"{chi}\", \"corr1\")    \n",
    "    # save_corr(BC, J, para.D_str[0], L, f\"P{Pdis}\", f\"{chi}\", \"corr2\")\n",
    "    # save_gap(BC, J, para.D_str[0], L, f\"P{Pdis}\", f\"{chi}\", \"energy\")   \n",
    "    # save_ZL(BC, J, para.D_str[0], L, f\"P{Pdis}\", f\"{chi}\", \"ZL\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538b528c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import timeit\n",
    "import sys\n",
    "import tarfile\n",
    "import datetime\n",
    "from multiprocessing import Pool\n",
    "import scriptCreator\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "def list_leaf_dirs(root_dir):\n",
    "    leaves = []\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        if not dirnames:\n",
    "            leaves.append(dirpath)\n",
    "        # print(dirpath)\n",
    "    return leaves\n",
    "\n",
    "def copy_folder(args):\n",
    "    Jstr = args\n",
    "    # print(\"args:\",Jstr)\n",
    "    root = f\"/homeold/aronton/tSDRG_random/tSDRG/Main_15/data_random/OBC/{Jstr}/Dim000/\"\n",
    "    for leaf in list_leaf_dirs(root):\n",
    "        leaf1 = leaf.replace('homeold', 'home').replace('data_random', 'data_random_old')\n",
    "        print(\"leaf:\", leaf)\n",
    "        shutil.copytree(leaf, leaf1, dirs_exist_ok=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    Jlist = [\"Jdis010\", \"Jdis020\", \"Jdis030\", \"Jdis040\", \"Jdis050\", \"Jdis060\", \"Jdis070\", \"Jdis080\", \"Jdis090\", \"Jdis100\",\n",
    "     \"Jdis110\", \"Jdis120\", \"Jdis130\", \"Jdis140\", \"Jdis150\", \"Jdis160\", \"Jdis170\", \"Jdis180\", \"Jdis190\", \"Jdis200\"]\n",
    "    Jlist = [\"Jdis050\", \"Jdis060\"]\n",
    "    def fun(arg):\n",
    "        with Pool() as pool:\n",
    "            pool.map(copy_folder, arg) \n",
    "        \n",
    "    # 計算函數執行時間\n",
    "    execution_time = timeit.timeit(lambda: fun(Jlist), number=1)\n",
    "\n",
    "    # 執行並顯示結果\n",
    "    # results1, results2 = fun(arg)\n",
    "    print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3973d03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def check(BC, J, D, L, P, m, phys, path):\n",
    "\n",
    "    path / BC / J / D / L / P / m / \n",
    "\n",
    "\n",
    "\n",
    "base_path = \"/home/aronton/tSDRG_random/tSDRG/Main_15/data_random/PBC/Jdis010/Dim002\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74065361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共儲存 200 筆結果到 matched_paths.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def collect_matching_paths(base_dir, output_file):\n",
    "    target_tags = [f\"_{i}\" for i in range(1000, 10001, 1000)]\n",
    "    records = []\n",
    "\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        for name in dirs + files:\n",
    "            for tag in target_tags:\n",
    "                if tag in name:\n",
    "                    val = int(tag[1:])\n",
    "                    full_path = os.path.join(root, name)\n",
    "                    records.append((val, full_path))\n",
    "                    break  # 一旦匹配其中一個 tag 就跳出\n",
    "\n",
    "    # 依數字大小排序（可省略）\n",
    "    records.sort(key=lambda x: x[0])\n",
    "\n",
    "    # 寫入文字檔\n",
    "    with open(output_file, 'w') as f:\n",
    "        for val, path in records:\n",
    "            f.write(f\"{val},{path}\\n\")\n",
    "\n",
    "    print(f\"共儲存 {len(records)} 筆結果到 {output_file}\")\n",
    "\n",
    "# 📂 設定資料夾與輸出檔\n",
    "base_path = \"/home/aronton/tSDRG_random/tSDRG/Main_15/data_random/PBC/Jdis010/Dim002\"\n",
    "output_path = \"matched_paths.csv\"  # 存在執行目錄\n",
    "\n",
    "# 🚀 執行\n",
    "collect_matching_paths(base_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14b10fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到最大目標：/home/aronton/tSDRG_random/tSDRG/Main_15/data_random/PBC/Jdis010/Dim002/L16_P10_m40_5000（數字：5000）\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def find_max_path_by_suffixes(base_dir):\n",
    "    # 建立 _1000 到 _10000 的字串列表\n",
    "    target_tags = [f\"_{i}\" for i in range(1000, 10001, 1000)]\n",
    "\n",
    "    max_val = -1\n",
    "    max_path = None\n",
    "\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        for name in dirs + files:  # 同時看資料夾與檔案\n",
    "            for tag in target_tags:\n",
    "                if tag in name:\n",
    "                    value = int(tag[1:])  # 去掉 \"_\" 轉成 int\n",
    "                    if value > max_val:\n",
    "                        max_val = value\n",
    "                        max_path = os.path.join(root, name)\n",
    "                    break  # 找到就跳出，避免多次匹配\n",
    "\n",
    "    return max_path, max_val\n",
    "\n",
    "# 📁 指定資料夾路徑\n",
    "base_path = \"/home/aronton/tSDRG_random/tSDRG/Main_15/data_random/PBC/Jdis010/Dim002\"\n",
    "\n",
    "# 🔍 執行\n",
    "path, val = find_max_path_by_suffixes(base_path)\n",
    "\n",
    "# 🖨️ 結果\n",
    "if path:\n",
    "    print(f\"找到最大目標：{path}（數字：{val}）\")\n",
    "else:\n",
    "    print(\"找不到 _1000 到 _10000 的匹配項目\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6efb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(BC, J, D, L, P, m, phys, path):\n",
    "    folder = folder_re.replace(\"L_re\", L).replace(\"P_re\", f\"P{P}\").replace(\"chi_re\", m)\n",
    "    name = name_re.replace(\"L_re\", L).replace(\"J_re\", J).replace(\"P_re\", f\"P{P}\").replace(\"chi_re\", m).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"dx_re\", f\"dx={L_num[l]-1}\")\n",
    "    path = Path(folder, J_list[j], D_list[d], name)  \n",
    "\n",
    "BC = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "79770d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10000, 9000, 8000, 7000, 6000, 5000, 4000, 3000, 2000, 1000]\n"
     ]
    }
   ],
   "source": [
    "a = list(range(1000, 10001, 1000))\n",
    "a.reverse()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5844ddfb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reverse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m a\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m,\u001b[38;5;241m10000\u001b[39m,\u001b[38;5;241m1000\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(reverse(a))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reverse' is not defined"
     ]
    }
   ],
   "source": [
    "a=list(range(1000,10000,1000))\n",
    "print(reverse(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ac4e55fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7d7cfcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(10000,1000,1000):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6b4b6518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aronton/tSDRG_random/tSDRG/Main_15/data_random/PBC/Jdis010/Dim002/L32_P10_m40_s_re/L32_P10_m40_1000_corr1.txt\n"
     ]
    }
   ],
   "source": [
    "def check(BC, J, D, L, P, m, phys, path):\n",
    "    folder = folder_re.replace(\"L_re\", L).replace(\"P_re\", f\"P{P}\").replace(\"chi_re\", m)\n",
    "    name = creatName(BC, \"Jdis010\", \"Dim002\", L, f\"P{P}\", \"m40\", \"corr1\")[1].replace(\"s_re\", \"1000\")\n",
    "    # name = name_re.replace(\"L_re\", L).replace(\"J_re\", J).replace(\"P_re\", f\"P{P}\").replace(\"chi_re\", m).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"dx_re\", f\"dx={L_num[l]-1}\")\n",
    "    path = Path(folder, name)\n",
    "    print(path)\n",
    "check(\"PBC\", \"Jdis010\", \"Dim002\", \"L32\", \"10\", \"m40\", \"corr1\", \"/home/aronton/tSDRG_random/tSDRG/Main_15/data_random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6bc42eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aronton/tSDRG_random/tSDRG/Main_15/data_random/PBC/Jdis010/Dim002/L32_P10_m40_1000\n",
      "L32_P10_m40_1000_corr1.txt\n",
      "/home/aronton/tSDRG_random/tSDRG/Main_15/data_random/PBC/Jdis010/Dim002/L32_P10_m40_1000/L32_P10_m40_1000_corr1.txt\n",
      "['x1,x2,corr\\n', '0,16,0.4676337859568083\\n', '1,17,0.614843786992552\\n', '2,18,0.6570779590296938\\n', '3,19,0.5381440064734961\\n', '4,20,0.6847521533740805\\n', '5,21,0.8681645920355825\\n', '6,22,0.8833595849874735\\n', '7,23,1.022252688834315\\n', '8,24,1.002041884548506\\n', '9,25,0.7811412471147365\\n', '10,26,0.5905479712856296\\n', '11,27,0.3992787930836768\\n', '12,28,0.3380841680219221\\n', '13,29,0.4532873641867693\\n', '14,30,0.4618304648040701\\n', '15,31,0.4085698008420554\\n']\n",
      "/home/aronton/tSDRG_random/tSDRG/Main_15/data_random/PBC/Jdis010/Dim002/L32_P10_m40_1000 L32_P10_m40_1000_corr1.txt /home/aronton/tSDRG_random/tSDRG/Main_15/data_random/PBC/Jdis010/Dim002/L32_P10_m40_1000/L32_P10_m40_1000_corr1.txt\n"
     ]
    }
   ],
   "source": [
    "BC = \"PBC\"\n",
    "J = \"Jdis010\" \n",
    "D = \"Dim002\"\n",
    "L = \"L32\"\n",
    "P = 10\n",
    "m = \"m40\"\n",
    "phys = \"corr1\"\n",
    "\n",
    "folder_re = creatDir(BC, \"Jdis010\", \"Dim002\", \"L32\", \"P10\", \"m40\", \"corr1\")[1]\n",
    "folder = folder_re.replace(\"L_re\", L).replace(\"P_re\", \"P10\").replace(\"chi_re\", m).replace(\"s_re\", \"1000\")\n",
    "print(folder)\n",
    "name = creatName(BC, \"Jdis010\", \"Dim002\", L, f\"P{P}\", \"m40\", \"corr1\")[1].replace(\"s_re\", \"1000\")\n",
    "print(name)\n",
    "# name = name_re.replace(\"L_re\", L).replace(\"J_re\", J).replace(\"P_re\", f\"P{P}\").replace(\"chi_re\", m).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"dx_re\", f\"dx=10\")\n",
    "path = Path(folder, name)  \n",
    "print(path)\n",
    "\n",
    "with path.open(\"r\") as f:\n",
    "    lines = f.readlines()\n",
    "print(lines)\n",
    "print(folder, name, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c192011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/aronton/tSDRG_random/tSDRG/Main_15/data_random/OBC/Jdis010/Dim002/L32_P10_m40_s_re'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creatDir(\"OBC\", \"Jdis010\", \"Dim002\", \"L32\", \"P10\", \"m40\", \"corr1\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "274ac293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('L_P_m_s_re_corr1.csv',\n",
       " 'L_P_m_s_re_corr1.txt',\n",
       " 'corr1_L_P_m_Jdis010_Dim002.txt',\n",
       " 'corr1_L_P_m_Jdis010_Dim002.txt',\n",
       " 'corr1_L_P_m_Jdis010_Dim002_dx_re.txt',\n",
       " 'corr1_dis_L_P_m_Jdis010_Dim002_dx_re.txt')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creatName(\"OBC\", \"Jdis010\", \"Dim002\", \"L\", \"P\", \"m\", \"corr1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c31a55c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis030/Dim000/L7_P10_m40/corr1_L7_P10_m40_Jdis030_Dim000.txt\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis030/Dim000/L7_P10_m40/corr1_L7_P10_m40_Jdis030_Dim000.txt\n",
      "Error: No data found for OBC, Jdis030, Dim000, L7, P10, m40, corr1\n",
      "False\n",
      "False\n",
      "False\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis080/Dim000/L7_P10_m40/corr1_L7_P10_m40_Jdis080_Dim000.txt\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis080/Dim000/L7_P10_m40/corr1_L7_P10_m40_Jdis080_Dim000.txt\n",
      "Error: No data found for OBC, Jdis080, Dim000, L7, P10, m40, corr1\n",
      "False\n",
      "False\n",
      "False\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis030/Dim000/L15_P10_m40/corr1_L15_P10_m40_Jdis030_Dim000.txt\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis030/Dim000/L15_P10_m40/corr1_L15_P10_m40_Jdis030_Dim000.txt\n",
      "Error: No data found for OBC, Jdis030, Dim000, L15, P10, m40, corr1\n",
      "False\n",
      "False\n",
      "False\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis080/Dim000/L15_P10_m40/corr1_L15_P10_m40_Jdis080_Dim000.txt\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis080/Dim000/L15_P10_m40/corr1_L15_P10_m40_Jdis080_Dim000.txt\n",
      "Error: No data found for OBC, Jdis080, Dim000, L15, P10, m40, corr1\n",
      "False\n",
      "False\n",
      "False\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis030/Dim000/L23_P10_m40/corr1_L23_P10_m40_Jdis030_Dim000.txt\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis030/Dim000/L23_P10_m40/corr1_L23_P10_m40_Jdis030_Dim000.txt\n",
      "Error: No data found for OBC, Jdis030, Dim000, L23, P10, m40, corr1\n",
      "False\n",
      "False\n",
      "False\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis080/Dim000/L23_P10_m40/corr1_L23_P10_m40_Jdis080_Dim000.txt\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis080/Dim000/L23_P10_m40/corr1_L23_P10_m40_Jdis080_Dim000.txt\n",
      "Error: No data found for OBC, Jdis080, Dim000, L23, P10, m40, corr1\n",
      "False\n",
      "False\n",
      "False\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis030/Dim000/L31_P10_m40/corr1_L31_P10_m40_Jdis030_Dim000.txt\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis030/Dim000/L31_P10_m40/corr1_L31_P10_m40_Jdis030_Dim000.txt\n",
      "Error: No data found for OBC, Jdis030, Dim000, L31, P10, m40, corr1\n",
      "False\n",
      "False\n",
      "False\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis080/Dim000/L31_P10_m40/corr1_L31_P10_m40_Jdis080_Dim000.txt\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis080/Dim000/L31_P10_m40/corr1_L31_P10_m40_Jdis080_Dim000.txt\n",
      "Error: No data found for OBC, Jdis080, Dim000, L31, P10, m40, corr1\n",
      "False\n",
      "False\n",
      "False\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis030/Dim000/L47_P10_m40/corr1_L47_P10_m40_Jdis030_Dim000.txt\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis030/Dim000/L47_P10_m40/corr1_L47_P10_m40_Jdis030_Dim000.txt\n",
      "Error: No data found for OBC, Jdis030, Dim000, L47, P10, m40, corr1\n",
      "False\n",
      "False\n",
      "False\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis080/Dim000/L47_P10_m40/corr1_L47_P10_m40_Jdis080_Dim000.txt\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis080/Dim000/L47_P10_m40/corr1_L47_P10_m40_Jdis080_Dim000.txt\n",
      "Error: No data found for OBC, Jdis080, Dim000, L47, P10, m40, corr1\n",
      "False\n",
      "False\n",
      "False\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis030/Dim000/L63_P10_m40/corr1_L63_P10_m40_Jdis030_Dim000.txt\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis030/Dim000/L63_P10_m40/corr1_L63_P10_m40_Jdis030_Dim000.txt\n",
      "Error: No data found for OBC, Jdis030, Dim000, L63, P10, m40, corr1\n",
      "False\n",
      "False\n",
      "False\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis080/Dim000/L63_P10_m40/corr1_L63_P10_m40_Jdis080_Dim000.txt\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis080/Dim000/L63_P10_m40/corr1_L63_P10_m40_Jdis080_Dim000.txt\n",
      "Error: No data found for OBC, Jdis080, Dim000, L63, P10, m40, corr1\n",
      "False\n",
      "False\n",
      "False\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis030/Dim000/L95_P10_m40/corr1_L95_P10_m40_Jdis030_Dim000.txt\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis030/Dim000/L95_P10_m40/corr1_L95_P10_m40_Jdis030_Dim000.txt\n",
      "Error: No data found for OBC, Jdis030, Dim000, L95, P10, m40, corr1\n",
      "False\n",
      "False\n",
      "False\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis080/Dim000/L95_P10_m40/corr1_L95_P10_m40_Jdis080_Dim000.txt\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis080/Dim000/L95_P10_m40/corr1_L95_P10_m40_Jdis080_Dim000.txt\n",
      "Error: No data found for OBC, Jdis080, Dim000, L95, P10, m40, corr1\n",
      "False\n",
      "False\n",
      "False\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis030/Dim000/L127_P10_m40/corr1_L127_P10_m40_Jdis030_Dim000.txt\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis030/Dim000/L127_P10_m40/corr1_L127_P10_m40_Jdis030_Dim000.txt\n",
      "Error: No data found for OBC, Jdis030, Dim000, L127, P10, m40, corr1\n",
      "False\n",
      "False\n",
      "False\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis080/Dim000/L127_P10_m40/corr1_L127_P10_m40_Jdis080_Dim000.txt\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis080/Dim000/L127_P10_m40/corr1_L127_P10_m40_Jdis080_Dim000.txt\n",
      "Error: No data found for OBC, Jdis080, Dim000, L127, P10, m40, corr1\n",
      "False\n",
      "False\n",
      "False\n",
      "Error reading file /home/aronton/tSDRG_random/Subpy/parameterRead/2025/2025_6_12/Spin15_L159_Jdis030_Dim000_P10_BC=OBC_chi4_partition=scopion3_seed1=1_seed2=5000_ds=100_task=submit_H2_M20_S59.txt: 's1'\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis030/Dim000/L159_P10_m40/corr1_L159_P10_m40_Jdis030_Dim000.txt\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis030/Dim000/L159_P10_m40/corr1_L159_P10_m40_Jdis030_Dim000.txt\n",
      "Error: No data found for OBC, Jdis030, Dim000, L159, P10, m40, corr1\n",
      "False\n",
      "False\n",
      "False\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis080/Dim000/L159_P10_m40/corr1_L159_P10_m40_Jdis080_Dim000.txt\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis080/Dim000/L159_P10_m40/corr1_L159_P10_m40_Jdis080_Dim000.txt\n",
      "Error: No data found for OBC, Jdis080, Dim000, L159, P10, m40, corr1\n",
      "False\n",
      "False\n",
      "False\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis030/Dim000/L191_P10_m40/corr1_L191_P10_m40_Jdis030_Dim000.txt\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis030/Dim000/L191_P10_m40/corr1_L191_P10_m40_Jdis030_Dim000.txt\n",
      "Error: No data found for OBC, Jdis030, Dim000, L191, P10, m40, corr1\n",
      "False\n",
      "False\n",
      "False\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis080/Dim000/L191_P10_m40/corr1_L191_P10_m40_Jdis080_Dim000.txt\n",
      "File not found: /home/aronton/tSDRG_random/tSDRG/Main_15/data_collect_old/OBC/Jdis080/Dim000/L191_P10_m40/corr1_L191_P10_m40_Jdis080_Dim000.txt\n",
      "Error: No data found for OBC, Jdis080, Dim000, L191, P10, m40, corr1\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import timeit\n",
    "import sys\n",
    "import tarfile\n",
    "import datetime\n",
    "import multiprocessing\n",
    "import scriptCreator\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "tSDRG_path = \"/home/aronton/tSDRG_random\"\n",
    "# group_path = \"/ceph/work/NTHU-qubit/LYT/tSDRG_random\"\n",
    "group_path = \"/home/aronton/tSDRG_random\"\n",
    "    \n",
    "sourcelist = {\"ZL\":\"ZL.csv\", \"energy\":\"energy.csv\", \"seed\":\"s_re_seed.csv\",\\\n",
    "    \"corr1\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"corr1.csv\"]), \"corr2\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"corr2.csv\"]),\\\n",
    "    \"ZLI\":\"ZLI.csv\", \"ZLC\":\"ZLC.csv\", \"w_loc\":\"w_loc.csv\", \"J_list\":\"J_list.csv\", \"dimerization\":\"dimerization.csv\",\\\n",
    "    \"string\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"string.csv\"])\n",
    "    }\n",
    "\n",
    "grouplist = {\"ZL\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"ZL.txt\"]), \"energy\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"energy.txt\"]),\\\n",
    "    \"corr1\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"corr1.txt\"]), \"corr2\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"corr2.txt\"]),\\\n",
    "    \"ZLI\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"ZLI.txt\"]), \"ZLC\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"ZLC.txt\"]),\\\n",
    "    \"w_loc\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"w_loc.txt\"]), \"J_list\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"J_list.txt\"]),\\\n",
    "    \"string\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"string.txt\"]), \"seed\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"seed.txt\"]),\\\n",
    "    \"dimerization\":\"_\".join([\"L_re\",\"P_re\",\"m_re\",\"s_re\",\"dimerization.txt\"])\n",
    "    }\n",
    "\n",
    "tarlist = {\n",
    "    \"ZL\":\"_\".join([\"ZL\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re.txt\"]),\\\n",
    "    \"energy\":\"_\".join([\"energy\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re.txt\"]),\\\n",
    "    \"corr1\":\"_\".join([\"corr1\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re.txt\"]),\\\n",
    "    \"corr2\":\"_\".join([\"corr2\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re.txt\"]),\\\n",
    "    \"ZLI\":\"_\".join([\"ZLI\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re.txt\"]),\\\n",
    "    \"ZLC\":\"_\".join([\"ZLC\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re.txt\"]),\\\n",
    "    \"w_loc\":\"_\".join([\"w_loc\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re.txt\"]),\\\n",
    "    \"J_list\":\"_\".join([\"J_list\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re.txt\"]),\\\n",
    "    \"string\":\"_\".join([\"string\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re.txt\"]),\\\n",
    "    \"seed\":\"_\".join([\"seed\",\"L_re\",\"P_re\",\"m_re.txt\"]),\n",
    "    \"dimerization\":\"_\".join([\"dimerization\",\"L_re\",\"P_re\",\"m_re.txt\"])\n",
    "    }\n",
    "\n",
    "metalist = {\n",
    "    \"ZL\":\"_\".join([\"ZL\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re.txt\"]),\\\n",
    "    \"gap\":\"_\".join([\"energy\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re.txt\"]),\\\n",
    "    \"corr1\":\"_\".join([\"corr1\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re\",\"dx_re.txt\"]),\\\n",
    "    \"corr2\":\"_\".join([\"corr2\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re\",\"dx_re.txt\"]),\\\n",
    "    }\n",
    "\n",
    "metaDislist = {\n",
    "    \"ZL\":\"_\".join([\"ZL_dis\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re.txt\"]),\\\n",
    "    \"gap\":\"_\".join([\"gap_dis\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re.txt\"]),\\\n",
    "    \"corr1\":\"_\".join([\"corr1_dis\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re\",\"dx_re.txt\"]),\\\n",
    "    \"corr2\":\"_\".join([\"corr2_dis\",\"L_re\",\"P_re\",\"m_re\",\"J_re\",\"D_re\",\"dx_re.txt\"]),\\\n",
    "    }\n",
    "\n",
    "\n",
    "def creatName(BC, J, D, L, P, m, phys):\n",
    "    mySourceName = sourcelist[phys].replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m) \n",
    "    myTargetName = tarlist[phys].replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m) \n",
    "    groupSourceName = grouplist[phys].replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m) \n",
    "    groupTargetName = tarlist[phys].replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m) \n",
    "    groupAveName = metalist[phys].replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m)\n",
    "    groupDisName = metaDislist[phys].replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m)\n",
    "    return (mySourceName, groupSourceName, myTargetName, groupTargetName, groupAveName, groupDisName)\n",
    "\n",
    "def creatCpName(BC, J, D, L, P, m, phys):\n",
    "    CpName = newlist[phys].replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m) \n",
    "    return CpName\n",
    "def creatColName(BC, J, D, L, P, m, phys):\n",
    "    colName = collist[phys].replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m) \n",
    "    return colName\n",
    "def creatDir(BC, J, D, L, P, m, phys):\n",
    "    avePath = \"/\".join([\"tSDRG\",\"Main_15\",\"metadata_old\",\"BC_re\",\"J_re\",\"D_re\",\"L_re_P_re_m_re_s_re\"])\n",
    "    sourcePath = \"/\".join([\"tSDRG\",\"Main_15\",\"data_random_old\",\"BC_re\",\"J_re\",\"D_re\",\"L_re_P_re_m_re_s_re\"])\n",
    "    tarPath = \"/\".join([\"tSDRG\",\"Main_15\",\"data_collect_old\",\"BC_re\",\"J_re\",\"D_re\",\"L_re_P_re_m_re\"])\n",
    "    mySourcePathBase = \"/\".join([tSDRG_path,sourcePath])\n",
    "    groupSourcePathBase = \"/\".join([group_path,sourcePath])\n",
    "    myTargetPathBase = \"/\".join([tSDRG_path,tarPath])\n",
    "    groupTargetPathBase = \"/\".join([group_path,tarPath])\n",
    "    avePathBase = \"/\".join([tSDRG_path,avePath])\n",
    "    # disPathBase = \"/\".join([tSDRG_path,avePath])\n",
    "    # sourcePathBase = f\"{tSDRG_path}/tSDRG/Main_15/data_random/BC_re/J_re/D_re/L_re_P_re_m_re_s_re/\"\n",
    "    # cpPathBase = f\"{group_path}/tSDRG/Main_15/data_random/BC_re/J_re/D_re/L_re_P_re_m_re_s_re/\"\n",
    "    # targetPathBase = f\"{group_path}/tSDRG/Main_15/data_collect/BC_re/J_re/D_re/L_re_P_re_m_re/\"\n",
    "\n",
    "    mySourcePath = mySourcePathBase.replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m)\n",
    "    groupSourcePath = groupSourcePathBase.replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m)\n",
    "    myTargetPath = myTargetPathBase.replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m)\n",
    "    groupTargetPath = groupTargetPathBase.replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m)\n",
    "    myAvePath = avePathBase.replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m).replace(\"s_re\", \"meta\")\n",
    "    myDisPath = avePathBase.replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m).replace(\"s_re\", \"dis\")\n",
    "    # cpPath = cpPathBase.replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m)\n",
    "    # targetPath = targetPathBase.replace(\"BC_re\", BC).replace(\"J_re\", J).replace(\"D_re\", D).replace(\"L_re\", L).replace(\"P_re\", P).replace(\"m_re\", m)\n",
    "    \n",
    "    return (mySourcePath, groupSourcePath, myTargetPath, groupTargetPath, myAvePath, myDisPath)\n",
    "\n",
    "def gapAverage(BC, J, D, L, P, m, phys):\n",
    "    folder = creatDir(BC, J, D, L, P, m, phys)\n",
    "    name = creatName(BC, J, D, L, P, m, phys)\n",
    "    myTarPath = folder[2] + \"/\" + name[2]\n",
    "    gaplist = []\n",
    "    with open(myTarPath, \"r\") as targetFile:\n",
    "        collect = targetFile.readlines()\n",
    "        collect = [line.strip() for line in collect]\n",
    "        if collect and collect[0] == \"energy\":\n",
    "            del collect[0]        \n",
    "        for line in collect:\n",
    "            line = line.split(\":\")[-1].split(\" \")\n",
    "            gaplist.append(line[1]-line[0])\n",
    "            \n",
    "def corrAverage(BC, J, D, L, P, m, phys):\n",
    "    folder = creatDir(BC, J, D, L, P, m, phys)\n",
    "    name = creatName(BC, J, D, L, P, m, phys)\n",
    "    myTarPath = folder[2] + \"/\" + name[2]\n",
    "\n",
    "    corrDic = {}\n",
    "    try:\n",
    "        with open(myTarPath, \"r\") as targetFile:\n",
    "            collect = targetFile.readlines()\n",
    "            collect = [line.strip() for line in collect]\n",
    "            if collect and collect[0] == \"corr1\":\n",
    "                del collect[0]\n",
    "            for line in collect:\n",
    "                line = line.split(\":\")[-1].split(\" \")\n",
    "                for data in line:\n",
    "                    if data:\n",
    "                        parts = data.split(\",\")\n",
    "                        if len(parts) >= 3:\n",
    "                            try:\n",
    "                                x1, x2 = int(parts[0]), int(parts[1])\n",
    "                                corr = float(parts[2])\n",
    "                                key = x2 - x1\n",
    "                                if key not in corrDic:\n",
    "                                    corrDic[key] = []\n",
    "                                corrDic[key].append(corr)\n",
    "                            except ValueError:\n",
    "                                continue  # 忽略無法轉換的數據\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {myTarPath}\")\n",
    "        return False, False, False\n",
    "    save_corrDistribute(corrDic, BC, J, D, L, P, m, phys)\n",
    "    corr = {}\n",
    "    sample = {}\n",
    "    error = {}\n",
    "    for key, values in corrDic.items():\n",
    "        sample[key] = len(values)\n",
    "        corr[key] = np.mean(values)\n",
    "        error[key] = np.std(values, ddof=1) if len(values) > 1 else 0.0\n",
    "\n",
    "    return corr, sample, error\n",
    "\n",
    "def save_corrDistribute(corrDic, BC, J, D, L, P, m, phys):\n",
    "    folder = creatDir(BC, J, D, L, P, m, phys)\n",
    "    name = creatName(BC, J, D, L, P, m, phys)\n",
    "    # print( f\"folder[4]:{folder[4]}\")\n",
    "    corrDisBase = folder[5] + \"/\" + name[5]\n",
    "    for key, values in corrDic.items():\n",
    "        context = \"\"\n",
    "        if BC == \"OBC\":\n",
    "            context = f\"C_etoe=<S(0)S(dx={key})>\\n\"\n",
    "        elif BC == \"PBC\":\n",
    "            context = f\"C_bulk=<S(0)S(dx={key})>\\n\"\n",
    "        corrDisPath = corrDisBase.replace(\"dx_re\", f\"dx={key}\")\n",
    "        for value in values:\n",
    "            context += f\"{value}\\n\"\n",
    "        \n",
    "        if context == f\"C_etoe=<S(0)S(dx={key})>\\n\" or context ==  f\"C_bulk=<S(0)S(dx={key})>\\n\":\n",
    "            continue\n",
    "        else:\n",
    "            if not os.path.exists(corrDisPath):\n",
    "                os.makedirs(os.path.dirname(corrDisPath), exist_ok=True)\n",
    "            with open(corrDisPath, \"w\") as targetFile:\n",
    "                targetFile.write(context)\n",
    "\n",
    "def save_corr(BC, J, D, L, P, m, phys):\n",
    "    corr, sample, error = corrAverage(BC, J, D, L, P, m, phys)\n",
    "    l = int(L.replace(\"L\", \"\"))\n",
    "    # print(\"l\",l)\n",
    "    # print(\"corr\",corr)\n",
    "    # print(corr[l-1], sample[l-1], error[l-1])\n",
    "    if corr == False:\n",
    "        print(f\"Error: No data found for {BC}, {J}, {D}, {L}, {P}, {m}, {phys}\")\n",
    "        return\n",
    "    folder = creatDir(BC, J, D, L, P, m, phys)\n",
    "    name = creatName(BC, J, D, L, P, m, phys)\n",
    "    # print( f\"folder[4]:{folder[4]}\")\n",
    "    myTarPathBase = folder[4] + \"/\" + name[4]\n",
    "    # print(f\"myTarPath:{myTarPath}\")\n",
    "\n",
    "    for key in corr.keys():\n",
    "        if corr[key] == None:\n",
    "            continue\n",
    "        myTarPath = myTarPathBase.replace(\"dx_re\", f\"dx={key}\")\n",
    "        if BC == \"OBC\":\n",
    "            context = f\"C_etoe=<S(0)S(dx={key})>,erro,sample\\n\" + f\"{corr[key]},{error[key]},{sample[key]}\"\n",
    "        elif BC == \"PBC\":\n",
    "            context = f\"C_bulk=<S(0)S(dx={key})>,erro,sample\\n\" + f\"{corr[key]},{error[key]},{sample[key]}\"\n",
    "        if not os.path.exists(myTarPath):\n",
    "            os.makedirs(os.path.dirname(myTarPath), exist_ok=True)\n",
    "        with open(myTarPath, \"w\") as targetFile:\n",
    "            targetFile.write(context)\n",
    "        # # print(key, corr[key], sample[key], error[key])\n",
    "        # print(f\"myTarPath:{myTarPath.replace(\"dx_re\", f\"dx={key}\")}\")\n",
    "        # with open(myTarPath, \"w\") as targetFile:\n",
    "        #     targetFile.write(\"key,corr,sample,error\\n\")\n",
    "\n",
    "\n",
    "# def save_ZL():\n",
    "    \n",
    "# def save_gap():\n",
    "\n",
    "def list_txt_files(directory):\n",
    "    txt_files = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                full_path = os.path.join(root, file)\n",
    "                txt_files.append(full_path)\n",
    "    return txt_files\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file = \"/home/aronton/tSDRG_random/Subpy/parameterRead/2025/2025_6_12\"\n",
    "    txt_files = list_txt_files(file)\n",
    "    # file = \"/home/aronton/tSDRG_random/Subpy/parameterRead/2025/2025_6_4/Spin15_L24_Jdis010_Dim000_P10_BC=PBC_chi40_partition=scopion1_seed1=1_seed2=10000_ds=50_task=submit_H12_M12_S6.txt\"\n",
    "    arg = []\n",
    "    # Jstr = [f\"Jdis{str(i).zfill(3)}\" for i in range(int(J),int(J)+1)]\n",
    "    # Jstr = [f\"Jdis{str(i).zfill(3)}\" for i in range(120,150,30)]\n",
    "\n",
    "    # Dstr = [f\"Dim{str(i).zfill(3)}\" for i in range(101)]\n",
    "    # Lstr = [f\"L{num}\" for num in range(31, 255, 32)]  # 只有 L512\n",
    "    # Lstr = [f\"L{num}\" for num in range(31, 127, 8)]  # 只有 L512\n",
    "    for file in txt_files:\n",
    "    # file = \"/home/aronton/tSDRG_random/Subpy/parameterRead/2025/2025_5_31/Spin15_L191_Jdis010_Dim000_P10_BC=OBC_chi40_partition=scopion3_seed1=1_seed2=5000_ds=100_task=submit_H21_M43_S55.txt\"\n",
    "        try:\n",
    "            a = scriptCreator.para(\"read\",file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file}: {e}\")\n",
    "            continue\n",
    "        parameterlist = a.para    \n",
    "        para=scriptCreator.paraList1(parameterlist[\"L\"],parameterlist[\"J\"],parameterlist[\"D\"],parameterlist[\"S\"])\n",
    "        BC = parameterlist[\"BC\"]\n",
    "        Pdis = parameterlist[\"Pdis\"]\n",
    "        chi = \"m\" + str(parameterlist[\"chi\"])\n",
    "        # s1 = int(parameterlist[\"S\"][\"S1\"])\n",
    "        # s2 = int(parameterlist[\"S\"][\"S2\"])\n",
    "        # s1 = int(sys.argv[2])\n",
    "        # s2 = int(sys.argv[3])\n",
    "        # BC = \"OBC\"\n",
    "        # Pdis = 10\n",
    "        # chi = \"m40\"\n",
    "        # Pdis = parameterlist[\"Pdis\"]\n",
    "        # chi = \"m\" + str(parameterlist[\"chi\"])\n",
    "        s1 = int(parameterlist[\"S\"][\"S1\"])\n",
    "        s2 = int(parameterlist[\"S\"][\"S2\"])\n",
    "        # corr, sample, error = corrAverage(BC, \"Jdis010\", \"Dim000\", \"L31\", \"P10\", \"m40\", \"corr1\")\n",
    "        # save_corr(BC, J, D, L, f\"P{Pdis}\", f\"{chi}\", \"corr1\")    \n",
    "        for s in [\"corr1\"]:\n",
    "            for L in para.L_str:\n",
    "                for J in para.J_str:\n",
    "                    corr, sample, error = corrAverage(BC, J, \"Dim000\", L, \"P10\", \"m40\", s)\n",
    "                    save_corr(BC, J, \"Dim000\", L, f\"P{Pdis}\", f\"{chi}\", \"corr1\")    \n",
    "                    print(corr)\n",
    "                    print(sample)\n",
    "                    print(error)\n",
    "                        # arg.append((BC, J, para.D_str[0], L, f\"P{Pdis}\", f\"{chi}\", s, 1, 10000))\n",
    "    # for s in [\"corr1\"]:\n",
    "    #     for L in para.L_str:\n",
    "    #         for J in para.J_str:\n",
    "    # arg.append((BC, J, D, L, f\"P{Pdis}\", f\"{chi}\", s, 1, 10000))\n",
    "    # print(arg)     \n",
    "    # corr, sample, error = corrAverage(BC, \"Jdis010\", \"Dim000\", \"L31\", \"P10\", \"m40\", \"corr1\")\n",
    "    # save_corr(BC, J, D, L, f\"P{Pdis}\", f\"{chi}\", \"corr1\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6a0c54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:x1,x2,corr 0,1,-3.138726698282046 0,2,2.091195460284133 0,3,-1.835099502286222 0,4,1.63334964113194 0,5,-1.370862052416925 0,6,1.484102492089434\n"
     ]
    }
   ],
   "source": [
    "myTarPath = \"/home/aronton/tSDRG_random/tSDRG/Main_15/data_collect/OBC/Jdis010/Dim000/L7_P10_m40/corr1_L7_P10_m40_Jdis010_Dim000.txt\"\n",
    "with open(myTarPath, \"r\") as targetFile:\n",
    "    collect = targetFile.readlines()\n",
    "    collect = [line.strip() for line in collect]  # Remove empty lines and strip whitespace\n",
    "print(collect[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d91faa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "-0.14058714223621982 5000 0.7500201012887087"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
